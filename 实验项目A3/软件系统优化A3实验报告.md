# 软件系统优化A3实验报告

# 环境配置

## 实验平台

本次的实验我进行的平台是台式机，CPU为***Intel 13600kf***，GPU为***NVIDIA GeForce RTX 4070 Ti SUPE***。

## cuda配置

由于之前进行过大模型的本地部署，因此安装过cuda，cuda版本如下<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114082249017.png" alt="image-20241114082249017" style="zoom: 44%;" />

## 安装opeapi-base-toolkit和one-api-for-nvidia-gpus

下载后根据GUI完成安装

![image-20241114082934406](/Users/harry/Library/Application Support/typora-user-images/image-20241114082934406.png)

## 安装Visual Studio并安装桌面c++开发环境

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114083025214.png" alt="image-20241114083025214" style="zoom:50%;" />

## 完成配置

完成上述的环境安装和配置后，我们运行环境配置脚本

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114084345496.png" alt="image-20241114084345496" style="zoom:50%;" />

成功运行

我们来编译并运行一下测试代码

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114084422798.png" alt="image-20241114084422798" style="zoom:50%;" />

程序成功选择了 ***NVIDIA GeForce RTX 4070 Ti SUPER*** 作为计算设备，并且通过 CUDA 后端进行计算。

# Part 1 实验课练习

## 任务1

学习***basic_parafor.cpp***代码，了解主机内存与设备内存（CPU、GPU）的分配、拷⻉、释放等函 数的应⽤，以及并⾏计算的`parallel_for`函数的使⽤

我们首先可以编译并运行一下***basic_parafor.cpp***

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114112703378.png" alt="image-20241114112703378" style="zoom:50%;" />

#### USM官方文档

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114104051651.png" alt="image-20241114104051651" style="zoom: 33%;" />

我们可以看到，官方将***USM***即***Undefined Shared memory***分为三种：

1. **host**
   **主机内存分配**，这些内存可以被 **设备** 访问（除了主机本身之外）。
   这种类型的内存分配是在主机（CPU）内存中进行的，但它同时允许设备（例如 GPU）访问这块内存。这种内存常用于主机和设备之间的数据传输或共享。

2. **device**
   **设备内存分配**，这些内存 **不能** 被主机访问。
   这种内存分配发生在设备内存中，例如 GPU 的显存，主机程序无法直接访问这些内存，只有设备（GPU 或其他加速器）能访问。设备内存通常用于存储需要在设备上处理的数据。

3. **shared**
   **共享内存分配**，这些内存可以被 **主机和设备** 共同访问。
   共享内存是一种特殊的内存类型，既能在主机（CPU）上使用，也能在设备（如 GPU）上使用。这种内存类型适用于主机和设备之间需要频繁交换数据的场景。

#### USM的分配

首先是分配主机内存的`sycl::malloc_host`

我们可以查看***SYCL官方文档***

##### `sycl::malloc_host`

```c++
template <typename T>
T* sycl::malloc_host(size_t count,
                     const sycl::context& syclContext,
                     const sycl::property_list& propList = {});

void* sycl::malloc_host(size_t numBytes,
                        const sycl::queue& syclQueue,
                        const sycl::property_list& propList = {});
```

我们可以看到***basic_parafor.cpp***使用的是**`sycl::malloc_host` with `sycl::queue`**中的`T* sycl::malloc_host`

对于参数，我们可以查看Parameters

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114092626280.png" alt="image-20241114092626280" style="zoom:33%;" />

使用`T* sycl::malloc_host`来分配内存需要我们给定两个必须参数，一个是`count`，用于给定元素的数量，另一个是`syclQueue`即分配内存的 SYCL 队列。

对于`syclQueu`，我们来继续查看文档

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114100023855.png" alt="image-20241114100023855" style="zoom:33%;" />

可以看到`sycl::queue`用于连接一个主机程序和一个单独的设备（实验中我们连接了GPU），我们可以通过其来进行提交命令组、监控任务完成、同步与异步。

因此通过***basic_parafor.cpp***中的

```c++
int *host_mem   = mallohc_host<int>(N, my_gpu_queue);
```

我们为GPU在主机内存中分配了N个int型大小的空间并获得了其指针。

##### `sycl::malloc_device`

```c++
void* sycl::malloc_device(size_t numBytes,
                          const sycl::device& syclDevice,
                          const sycl::context& syclContext,
                          const sycl::property_list& propList = {});

template <typename T>
T* sycl::malloc_device(size_t count,
                       const sycl::device& syclDevice,
                       const sycl::context& syclContext,
                       const sycl::property_list& propList = {});

void* sycl::malloc_device(size_t numBytes,
                          const sycl::queue& syclQueue,
                          const sycl::property_list& propList = {});
```

参数与使用方法和`sycl::malloc_host`相同

因此我们通过

```c++
int *device_mem = malloc_device<int>(N, my_gpu_queue);
```

为GPU在GPU内存中分配了N个int型大小的空间并获得了其指针。需要注意的其分配的内存只能由设备（本实验中为GPU）直接访问，而不能直接由主机程序访问。

#### USM的拷贝

官方文档如下

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114103729388.png" alt="image-20241114103729388" style="zoom:33%;" />

可见该函数用于在USM拷贝内存，我们需给定参数`dest`和`src`指针，并给定拷贝数据的大小。

该函数在***basic_parafor.cpp***中使用如下

```c++
// Copy from host(CPU) to device(GPU)
my_gpu_queue.memcpy(device_mem, host_mem, N * sizeof(int)).wait();
```

这里的`wait`指的是等待拷贝完成后再继续执行接下来的代码。

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114110533737.png" alt="image-20241114110533737" style="zoom:33%;" />

如此我们将`host`内存中的内容拷贝到了`device`内存中。

####  USM的释放

官方文档如下

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114111049488.png" alt="image-20241114111049488" style="zoom:33%;" />

我们可以发现该函数有两种使用方式

一种是传入指定了内存分配时使用的上下文`syclContext` &`sycl::context` 

另一种是传入传入队列对象`syclQueue` & `sycl::queue` 

显然在***basic_parafor.cpp***中我们使用第二种

```c++
free(host_mem, my_gpu_queue);
free(device_mem, my_gpu_queue); 
```



## 任务2

修改basic_parafor.cpp代码，将申请的内存空间修改为本机与设备共享内存空间。并思考主机内 存和设备内存修改为共享内存空间的好处。

#### 修改代码为共享内存空间

我们首先通过官方文档来看怎么来申请共享内存空间

##### `sycl::malloc_shared`

```c++
void* sycl::malloc_shared(size_t numBytes,
                          const sycl::device& syclDevice,
                          const sycl::context& syclContext,
                          const sycl::property_list& propList = {});

template <typename T>
T* sycl::malloc_shared(size_t count,
                       const sycl::device& syclDevice,
                       const sycl::context& syclContext,
                       const sycl::property_list& propList = {});

void* sycl::malloc_shared(size_t numBytes,
                          const sycl::queue& syclQueue,
                          const sycl::property_list& propList = {});

template <typename T>
T* sycl::malloc_shared(size_t count,
                       const sycl::queue& syclQueue,
                       const sycl::property_list& propList = {});

void* sycl::aligned_alloc_shared(size_t alignment, size_t numBytes,
                                 const sycl::device& syclDevice,
                                 const sycl::context& syclContext,
                                 const sycl::property_list& propList = {});

template <typename T>
T* sycl::aligned_alloc_shared(size_t alignment, size_t count,
                              const sycl::device& syclDevice,
                              const sycl::context& syclContext,
                              const sycl::property_list& propList = {});

void* sycl::aligned_alloc_shared(size_t alignment, size_t numBytes,
                                 const sycl::queue& syclQueue,
                                 const sycl::property_list& propList = {});

template <typename T>
T* sycl::aligned_alloc_shared(size_t alignment, size_t count,
                              const sycl::queue& syclQueue,
                              const sycl::property_list& propList = {});
```

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114111748225.png" alt="image-20241114111748225" style="zoom:33%;" />

使用方法类似，我们只需要给定申请的元素类型、元素个数以及SYCL队列，同时我们也不必再对内存数据进行拷贝。注意最后需要释放内存。如此我们代码修改如下

```c++
#include <sycl/sycl.hpp>
#include <iostream>
using namespace sycl;

constexpr int N = 10;

int main() {
    queue my_gpu_queue(sycl::gpu_selector_v);

    std::cout << "Selected GPU device: " <<
        my_gpu_queue.get_device().get_info<info::device::name>() << "\n";

    // 申请共享内存，主机和设备共享此内存
    int *shared_mem = malloc_shared<int>(N, my_gpu_queue);

    for (int i = 0; i < N; i++) {
        shared_mem[i] = i;
    }

    my_gpu_queue.submit([&](handler& h) {

        h.parallel_for(range{N}, [=](id<1> item) {
            shared_mem[item] *= 2; 
        });

    });

    my_gpu_queue.wait();

    printf("\nData Result\n");
    for (int i = 0; i < N; i++) {
        printf("%d, ", shared_mem[i]);
    }
    printf("\nTask Done!\n");

    free(shared_mem, my_gpu_queue);

    return 0;
}

```

我们进行编译并运行

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114112723832.png" alt="image-20241114112723832" style="zoom: 50%;" />

成功完成修改

#### 共享内存的好处

共享内存是主机和设备之间的一种高效数据交换机制，它的主要好处包括：

- 提高数据访问效率和减少延迟。
- 简化主机和设备之间的数据传输。
- 减少内存开销并优化内存使用。
- 提升并行计算性能。
- 增强设备资源的利用率。
- 简化数据管理和内存分配。

共享内存不仅适用于主机和设备之间的数据传输，还大大提高了计算性能，尤其是在需要并行计算的场景中。

这里我们可以进行一个对比测试，我们结合一下前面的两种内存的代码，并计时

```c++
#include <sycl/sycl.hpp>
#include <iostream>
#include <chrono>

using namespace sycl;
constexpr int N = 10;

void test_malloc_host_device(queue& my_gpu_queue) {
    auto start = std::chrono::high_resolution_clock::now();

    int* host_mem = malloc_host<int>(N, my_gpu_queue);
    int* device_mem = malloc_device<int>(N, my_gpu_queue);

    // Init CPU data
    for (int i = 0; i < N; i++) {
        host_mem[i] = i;
    }

    // Copy from host(CPU) to device(GPU)
    my_gpu_queue.memcpy(device_mem, host_mem, N * sizeof(int)).wait();

    // Submit the content to the queue for execution
    my_gpu_queue.submit([&](handler& h) {
        h.parallel_for(range{N}, [=](id<1> item) {
            device_mem[item] *= 2;
        });
    });

    // Wait the computation done
    my_gpu_queue.wait();

    // Copy back from GPU to CPU
    my_gpu_queue.memcpy(host_mem, device_mem, N * sizeof(int)).wait();

    printf("\nMalloc Host & Device Results:\n");
    for (int i = 0; i < N; i++) {
        printf("%d, ", host_mem[i]);
    }
    printf("\n");

    free(host_mem, my_gpu_queue);
    free(device_mem, my_gpu_queue);

    auto end = std::chrono::high_resolution_clock::now();
    std::cout << "Malloc Host & Device Execution Time: "
              << std::chrono::duration<double, std::milli>(end - start).count() << " ms\n";
}

void test_malloc_shared(queue& my_gpu_queue) {
    auto start = std::chrono::high_resolution_clock::now();

    int* shared_mem = malloc_shared<int>(N, my_gpu_queue);

    // Init data
    for (int i = 0; i < N; i++) {
        shared_mem[i] = i;
    }

    // Submit task to the queue
    my_gpu_queue.submit([&](handler& h) {
        h.parallel_for(range{N}, [=](id<1> item) {
            shared_mem[item] *= 2;
        });
    });

    // Wait the computation done
    my_gpu_queue.wait();

    printf("\nMalloc Shared Results:\n");
    for (int i = 0; i < N; i++) {
        printf("%d, ", shared_mem[i]);
    }
    printf("\n");

    free(shared_mem, my_gpu_queue);

    auto end = std::chrono::high_resolution_clock::now();
    std::cout << "Malloc Shared Execution Time: "
              << std::chrono::duration<double, std::milli>(end - start).count() << " ms\n";
}

int main() {
    queue my_gpu_queue(sycl::gpu_selector_v);

    std::cout << "Selected GPU device: "
              << my_gpu_queue.get_device().get_info<info::device::name>() << "\n";

    test_malloc_host_device(my_gpu_queue);
    test_malloc_shared(my_gpu_queue);

    return 0;
}
```

可以看到结果确实共享内存更高效

<img src="../../../Library/Application Support/typora-user-images/image-20241118225058293.png" alt="image-20241118225058293" style="zoom:50%;" />

## 任务3

创建⼀个新⽂件（vector_add.cpp)，使⽤ND_range实现⼀个向量加法程序

### nd_range

首先我们查看什么是nd_range

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114141941647.png" alt="image-20241114141941647" style="zoom:33%;" />

据此我们可以得知，通过两个`sycl::range`参数，`sycl::nd_range` 可以表示如何将整个计算任务分解成多个工作组，以及每个工作组内部的任务如何分配和调度。

### 代码实现

接下来我们来开始完成代码。

我们选用的方案是，分配host和device内存，拷贝数据后进行运算，最后将结果拷贝回来并释放内存。这里我们按序初始化了向量用于vector_add的运算。

```c++
#include <sycl/sycl.hpp>
#include <iostream>
using namespace sycl;

constexpr int N = 1024;  // 向量的大小

int main() {
    // 创建队列，选择 GPU 设备
    queue my_gpu_queue(sycl::gpu_selector_v);

    std::cout << "Selected GPU device: " <<
        my_gpu_queue.get_device().get_info<info::device::name>() << "\n";

    // 在主机上创建并初始化向量
    int *A = malloc_host<int>(N, my_gpu_queue);
    int *B = malloc_host<int>(N, my_gpu_queue);
    int *C = malloc_host<int>(N, my_gpu_queue);

    for (int i = 0; i < N; i++) {
        A[i] = i;   // 向量A的元素初始化为索引值
        B[i] = 2*i; // 向量B的元素初始化为2倍的索引值
    }

    // 在设备上分配内存
    int *d_A = malloc_device<int>(N, my_gpu_queue);
    int *d_B = malloc_device<int>(N, my_gpu_queue);
    int *d_C = malloc_device<int>(N, my_gpu_queue);

    // 将数据从主机传输到设备
    my_gpu_queue.memcpy(d_A, A, N * sizeof(int)).wait();
    my_gpu_queue.memcpy(d_B, B, N * sizeof(int)).wait();

    // 定义工作组大小和全局工作大小
    size_t local_size = 256;    // 每个工作组中的元素数
    size_t global_size = N;     // 全局工作项数量

    // 使用ND_range提交向量加法计算
    my_gpu_queue.submit([&](handler& h) {
        // 设置设备内存的访问权限
        h.parallel_for<nd_range<1>>(nd_range<1>(global_size, local_size), [=](nd_item<1> item) {
            // 获取当前工作项的索引
            size_t idx = item.get_global_id(0);
            
            if (idx < N) {  // 确保索引在合法范围内
                d_C[idx] = d_A[idx] + d_B[idx];  // 向量加法
            }
        });
    });

    // 等待GPU计算完成
    my_gpu_queue.wait();

    // 将结果从设备拷贝回主机
    my_gpu_queue.memcpy(C, d_C, N * sizeof(int)).wait();

    // 输出结果
    std::cout << "First 10 elements of the result vector C:\n";
    for (int i = 0; i < 10; i++) {
        std::cout << C[i] << ", ";
    }
    std::cout << "\n";

    // 释放内存
    free(A, my_gpu_queue);
    free(B, my_gpu_queue);
    free(C, my_gpu_queue);
    free(d_A, my_gpu_queue);
    free(d_B, my_gpu_queue);
    free(d_C, my_gpu_queue);

    return 0;
}

```

然后我们编译并运行

<img src="/Users/harry/Library/Application Support/typora-user-images/image-20241114142816897.png" alt="image-20241114142816897" style="zoom:50%;" />

完成vector_add。

# Part 2 课后作业



### 作业1 

改写gemm_basic代码26、27⾏，利⽤work group和local work item的坐标来计算global坐标



#### 代码改写

我们修改代码，将原本的直接调用`get_global_id()`修改为计算`group_id*block_size+local_id`得到即可

```c++
 // 获取当前局部工作项和工作组的坐标
int local_row = index.get_local_id(0);
int local_col = index.get_local_id(1);
int group_row = index.get_group(0);
int group_col = index.get_group(1);

// 计算全局坐标
int row = group_row * block_size + local_row;
int col = group_col * block_size + local_col;
              
```

#### 测试代码

修改后编译发现，存在报错

<img src="../../../Library/Application Support/typora-user-images/image-20241118215103882.png" alt="image-20241118215103882" style="zoom:50%;" />

第一处报错`fabs`存在歧义，我们修改为`std::fabs`。

第二出为`cl::sycl`已经被改为`sycl`，修改即可。

编译并运行

<img src="../../../Library/Application Support/typora-user-images/image-20241118220408755.png" alt="image-20241118220408755" style="zoom:50%;" />

可以看到我们成功完成作业1



### 作业2

修改程序输⼊数据的⼤⼩，设定为M=N=K=2000，修改程序，并使其通过正确性测试。

#### 代码改写

只需改写输入的mnk即可

#### 测试代码

![image-20241118220555698](../../../Library/Application Support/typora-user-images/image-20241118220555698.png)

成功完成，我们可以发现，GPU在大规模的矩阵运算上有着明显的优势。